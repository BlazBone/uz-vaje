{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Reduction of dimensionality and recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Direct PCA method\n",
    "\n",
    "The primary purpose of the PCA method is reduction of dimensionality. This means that\n",
    "we want to find such a linear transformation of the input data that will describe the data\n",
    "in a low dimensional orthogonal space (which can also be called a subspace) while losing\n",
    "the minimal amount of information needed to reconstruct the data. A subspace is defined\n",
    "by its basis vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: The dual PCA method\n",
    "\n",
    "When analyzing image data, the dimensionality can become very large (much larger than\n",
    "the number of elements in the input), the direct method of computing eigenvectors we\n",
    "used previously is no longer suitable. E.g. if we have 10000-dimensional data, this would\n",
    "produce a covariance matrix of size 10000\u000210000. Here we are close to hitting the limits of\n",
    "computer memory. As the number of data samples N is lower than the dimensionality we\n",
    "can use the dual PCA method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Image decomposition examples\n",
    "\n",
    "Here we will use the dual PCA method on a computer vision problem. In the supplementary\n",
    "material there are three series of images. Each series contains 64 images of a face\n",
    "under different lighting conditions. Your task will be to analyze each series using the PCA\n",
    "method."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
